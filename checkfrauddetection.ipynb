{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpYzsivSDf0iHnU2lcmlbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumitt1/pythonall/blob/master/checkfrauddetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "def preprocess_image(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    resized = cv2.resize(blurred, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "    return resized\n",
        "\n",
        "# Step 2: Signature Extraction\n",
        "def extract_signature(image):\n",
        "    # Implement your signature extraction logic here\n",
        "    # For example, if the signature is in a fixed location:\n",
        "    signature = image[100:300, 400:600]\n",
        "    return signature\n",
        "\n",
        "# Step 3: Signature Comparison\n",
        "def compare_signatures(signature_image, reference_signature):\n",
        "    # Implement your signature comparison logic here\n",
        "    # For example, using structural similarity index (SSIM):\n",
        "    gray1 = cv2.cvtColor(signature_image, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(reference_signature, cv2.COLOR_BGR2GRAY)\n",
        "    (score, diff) = cv2.compareSSIM(gray1, gray2, full=True)\n",
        "    return score\n",
        "\n",
        "# Step 4: Amount and MICR Verification\n",
        "def extract_amount(image):\n",
        "    # Implement OCR or pattern matching logic to extract the amount\n",
        "    # Example using pytesseract OCR:\n",
        "    amount = pytesseract.image_to_string(image, config='--psm 7')\n",
        "    return amount\n",
        "\n",
        "def extract_MICR(image):\n",
        "    # Implement OCR or pattern matching logic to extract the MICR\n",
        "    # Example using pytesseract OCR:\n",
        "    micr = pytesseract.image_to_string(image, config='--psm 7')\n",
        "    return micr\n",
        "\n",
        "# Step 5: Watermark Analysis\n",
        "def check_watermark_presence(image):\n",
        "    # Implement watermark detection logic\n",
        "    # Example: Check if a specific watermark pattern exists\n",
        "    # You may use image processing techniques like template matching\n",
        "    # or machine learning-based methods to detect the watermark.\n",
        "    # Return True if the watermark is present, False otherwise.\n",
        "    return False\n",
        "\n",
        "# Step 6: Tampering Detection\n",
        "def detect_tampering(image):\n",
        "    # Implement tampering detection logic\n",
        "    # Example: Check for changes in specific regions of the cheque\n",
        "    # You can compare pixel values, analyze texture, or use other techniques.\n",
        "    # Return True if tampering is detected, False otherwise.\n",
        "    return False\n",
        "\n",
        "# Load the cheque image using cv2.imread()\n",
        "cheque_image = cv2.imread(\"cheque_image.jpg\")\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "preprocessed_image = preprocess_image(cheque_image)\n",
        "\n",
        "# Step 2: Signature Extraction\n",
        "signature_image = extract_signature(preprocessed_image)\n",
        "\n",
        "# Step 3: Signature Comparison\n",
        "reference_signature = cv2.imread(\"reference_signature.jpg\")\n",
        "similarity_score = compare_signatures(signature_image, reference_signature)\n",
        "\n",
        "# Step 4: Amount and MICR Verification\n",
        "extracted_amount = extract_amount(cheque_image)\n",
        "extracted_MICR = extract_MICR(cheque_image)\n",
        "\n",
        "# Step 5: Watermark Analysis\n",
        "watermark_presence = check_watermark_presence(cheque_image)\n",
        "\n",
        "# Step 6: Tampering Detection\n",
        "tampering_detected = detect_tampering(cheque_image)\n",
        "\n",
        "# Output results\n",
        "print(\"Signature similarity score:\", similarity_score)\n",
        "print(\"Extracted amount:\", extracted_amount)\n",
        "print(\"Extracted MICR:\", extracted_MICR)\n",
        "print(\"Watermark presence:\", watermark_presence)\n",
        "print(\"Tampering detected:\", tampering_detected)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "H0pyh9CVnDVb",
        "outputId": "10c4abbd-6375-40f1-8a7f-fa174110184f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-106e2e20a8b2>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Step 1: Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mpreprocessed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheque_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Step 2: Signature Extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-106e2e20a8b2>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Step 1: Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mblurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblurred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZMNmQYenejS",
        "outputId": "8b763948-be60-4168-f482-cadf80da4a22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (8.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration File: aws_iot_config.ini\n",
        "\n",
        "[AWS]\n",
        "access_key = AWS_ACCESS_KEY\n",
        "secret_key = AWS_SECRET_KEY\n",
        "region = AWS_REGION\n",
        "\n",
        "[IoT_Device]\n",
        "device_id = IOT_DEVICE_ID\n",
        "certificate_file = device_certificate.pem\n",
        "private_key_file = device_private_key.pem\n",
        "root_ca_file = root_ca.pem\n"
      ],
      "metadata": {
        "id": "mpNAOAuj4mZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "\n",
        "# Read the configuration file\n",
        "config = configparser.ConfigParser()\n",
        "config.read('aws_iot_config.ini')\n",
        "\n",
        "# Access AWS credentials\n",
        "aws_access_key = config.get('AWS', 'access_key')\n",
        "aws_secret_key = config.get('AWS', 'secret_key')\n",
        "aws_region = config.get('AWS', 'region')\n",
        "\n",
        "# Access IoT device details\n",
        "device_id = config.get('IoT_Device', 'device_id')\n",
        "certificate_file = config.get('IoT_Device', 'certificate_file')\n",
        "private_key_file = config.get('IoT_Device', 'private_key_file')\n",
        "root_ca_file = config.get('IoT_Device', 'root_ca_file')\n",
        "\n",
        "\n",
        "# Retrieve data file from S3\n",
        "bucket_name = 'your-bucket-name'\n",
        "object_key = 'path/to/your/data/file.csv'\n",
        "response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
        "data = response['Body'].read().decode('utf-8')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(data)\n"
      ],
      "metadata": {
        "id": "Tl8VTsR_4hl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('path/to/your/data/file.csv')\n",
        "\n",
        "# Separate the features and labels\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "XabYn8k_4aob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d-Uk3VZ64dko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}